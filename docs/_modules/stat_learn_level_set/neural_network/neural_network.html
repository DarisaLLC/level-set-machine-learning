
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>stat_learn_level_set.neural_network.neural_network &#8212; LIDC Dataset and Segmentation  documentation</title>
    <link rel="stylesheet" href="../../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" src="../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for stat_learn_level_set.neural_network.neural_network</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">cPickle</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">stats_recorder</span> <span class="k">as</span> <span class="nn">sr</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="k">import</span> <span class="n">datetime</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="k">import</span> <span class="n">fmin_l_bfgs_b</span> <span class="k">as</span> <span class="n">fmin</span>

<div class="viewcode-block" id="neural_network"><a class="viewcode-back" href="../../../slls/api/neural_network/neural_network.html#stat_learn_level_set.neural_network.neural_network.neural_network">[docs]</a><span class="k">class</span> <span class="nc">neural_network</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Single hidden layer neural network with single output unit that</span>
<span class="sd">    computes the identity.</span>

<span class="sd">    params: W, where w[i,j] = weight from input i to hidden unit j.</span>
<span class="sd">            b, where b[j] = bias into hidden unit j.</span>
<span class="sd">            z, where z[j] = weight from hidden unit j to output unit.</span>
<span class="sd">            c, single scalar bias for output unit.</span>

<span class="sd">    For a single vector input, the computation chain is:</span>
<span class="sd">    h = hidden_layer = tanh( dot(W.T, input) + b )</span>
<span class="sd">    output = dot(h, z) + c</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ninput</span><span class="p">,</span> <span class="n">nhidden</span><span class="p">,</span> <span class="n">rs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reg</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        ninput: int</span>
<span class="sd">            Number of input units.</span>
<span class="sd"> </span>
<span class="sd">        nhidden: int</span>
<span class="sd">            Number of hidden units.</span>

<span class="sd">        rs: numpy.random.RandomState, default=None</span>
<span class="sd">            Provide a RandomState object for reproducible results.</span>

<span class="sd">        reg: float, default=0</span>
<span class="sd">            The amount of L2 regularization of model parameters </span>
<span class="sd">            to add to the loss function.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ni</span> <span class="o">=</span> <span class="n">ninput</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nh</span> <span class="o">=</span> <span class="n">nhidden</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">()</span> <span class="k">if</span> <span class="n">rs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">rs</span>

        <span class="c1"># Initialize a stats_recorder to keep track of sample </span>
        <span class="c1"># mean and standard deviation of observations.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stats</span> <span class="o">=</span> <span class="n">sr</span><span class="o">.</span><span class="n">stats_recorder</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">=</span> <span class="n">reg</span>

        <span class="c1"># This both intializes and randomizes.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">randomize_params</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;&lt;neural_network ninput=</span><span class="si">%d</span><span class="s2">, nhidden=</span><span class="si">%d</span><span class="s2">&gt;&quot;</span><span class="o">%</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ni</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nh</span><span class="p">)</span>

<div class="viewcode-block" id="neural_network.randomize_params"><a class="viewcode-back" href="../../../slls/api/neural_network/neural_network.html#stat_learn_level_set.neural_network.neural_network.neural_network.randomize_params">[docs]</a>    <span class="k">def</span> <span class="nf">randomize_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Randomize the model parameters using IID Gaussian random variables</span>
<span class="sd">        with variance `scale`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">rs</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ni</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nh</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">rs</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nh</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">rs</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nh</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">rs</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span></div>

<div class="viewcode-block" id="neural_network.get_params"><a class="viewcode-back" href="../../../slls/api/neural_network/neural_network.html#stat_learn_level_set.neural_network.neural_network.neural_network.get_params">[docs]</a>    <span class="k">def</span> <span class="nf">get_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">flat</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        flat: bool, default=False</span>
<span class="sd">            If True, the parameters are flattened into a single array.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        params: list or array</span>
<span class="sd">            If `flat` is False (default), then the parameters are returned </span>
<span class="sd">            as [W,b,z,c]. Otherwise, these are flattened into a single </span>
<span class="sd">            array and returned.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">flat</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
                              <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params</span><span class="p">()])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c</span><span class="p">]</span></div>

<div class="viewcode-block" id="neural_network.set_params"><a class="viewcode-back" href="../../../slls/api/neural_network/neural_network.html#stat_learn_level_set.neural_network.neural_network.neural_network.set_params">[docs]</a>    <span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the parameter values to those providedin the arguments.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ni</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nh</span><span class="p">),</span> <span class="s2">&quot;W is wrong shape.&quot;</span>
        <span class="k">assert</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nh</span><span class="p">,),</span>         <span class="s2">&quot;b is wrong shape.&quot;</span>
        <span class="k">assert</span> <span class="n">z</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nh</span><span class="p">,),</span>         <span class="s2">&quot;z is wrong shape.&quot;</span>
        <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">c</span><span class="p">),</span>                <span class="s2">&quot;c should be a scalar.&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">W</span><span class="p">;</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="p">;</span> <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">c</span></div>

<div class="viewcode-block" id="neural_network.save"><a class="viewcode-back" href="../../../slls/api/neural_network/neural_network.html#stat_learn_level_set.neural_network.neural_network.neural_network.save">[docs]</a>    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="s2">&quot;./&quot;</span><span class="p">,</span> <span class="n">fname</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ts</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Pickle the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">fname</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">ts</span><span class="p">:</span>
            <span class="n">fname</span> <span class="o">=</span> <span class="s2">&quot;nnet-model-</span><span class="si">%d</span><span class="s2">-</span><span class="si">%d</span><span class="s2">-</span><span class="si">%s</span><span class="s2">.pkl&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ni</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nh</span><span class="p">,</span>
                                                 <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">())</span>
        <span class="k">elif</span> <span class="n">fname</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">ts</span><span class="p">:</span>
            <span class="n">fname</span> <span class="o">=</span> <span class="s2">&quot;nnet-model-</span><span class="si">%d</span><span class="s2">-</span><span class="si">%d</span><span class="s2">.pkl&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ni</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nh</span><span class="p">)</span>

        <span class="n">fpath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base</span><span class="p">,</span> <span class="n">fname</span><span class="p">)</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fpath</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span> <span class="n">cPickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span></div>

<div class="viewcode-block" id="neural_network.predict"><a class="viewcode-back" href="../../../slls/api/neural_network/neural_network.html#stat_learn_level_set.neural_network.neural_network.neural_network.predict">[docs]</a>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X: ndarray, shape=(nsamples, ninputs)</span>
<span class="sd">            Each row of `X` is an observation.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        out: ndarray, shape=(nsamples,)</span>
<span class="sd">            out = dot(tanh(X,W)+b, z) + c</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">nobservations</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">Z</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">std</span>
            <span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>
            
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">c</span> </div>

<div class="viewcode-block" id="neural_network.loss"><a class="viewcode-back" href="../../../slls/api/neural_network/neural_network.html#stat_learn_level_set.neural_network.neural_network.neural_network.loss">[docs]</a>    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the mean-squared-error loss between the network&#39;s </span>
<span class="sd">        prediction of `X` and the values `y`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X: ndarray, shape=(nsamples, ninput)</span>
<span class="sd">            The input array.</span>

<span class="sd">        y: ndarray, shape=(nsamples,)</span>
<span class="sd">            The response variable, i.e., the &quot;correct&quot; output values.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        mse: float</span>
<span class="sd">            The mean-squared error between the network ouput with `X` as input</span>
<span class="sd">            and `y`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">diff</span><span class="p">,</span> <span class="n">diff</span><span class="p">)</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="mf">0.5</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">reg</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>  <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">ni</span> <span class="o">+</span>\
                             <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">nh</span>
        <span class="k">return</span> <span class="n">loss</span></div>

<div class="viewcode-block" id="neural_network.lossgrad"><a class="viewcode-back" href="../../../slls/api/neural_network/neural_network.html#stat_learn_level_set.neural_network.neural_network.neural_network.lossgrad">[docs]</a>    <span class="k">def</span> <span class="nf">lossgrad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return loss and [Dloss/Dparam for param in [W,b,z,c]] </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">nsamples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Normalize if we are able.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">nobservations</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">Z</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">std</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">Z</span> <span class="o">=</span> <span class="n">X</span>

        <span class="c1"># Compute the network&#39;s prediction.</span>
        <span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>
        <span class="n">o</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">c</span>

        <span class="n">diff</span> <span class="o">=</span> <span class="n">o</span><span class="o">-</span><span class="n">y</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">diff</span><span class="p">,</span> <span class="n">diff</span><span class="p">)</span> <span class="o">/</span> <span class="n">nsamples</span>

        <span class="c1"># Add self.regularization penalty to loss.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="mf">0.5</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">reg</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>  <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">ni</span> <span class="o">+</span>\
                             <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">nh</span>

        <span class="c1"># Gradient wrt to bias term in &quot;hidden =&gt; output&quot;.</span>
        <span class="n">Dc</span> <span class="o">=</span> <span class="n">diff</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="n">Htilde</span> <span class="o">=</span> <span class="n">diff</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">H</span>

        <span class="c1"># Gradient wrt linear coefs in &quot;hidden =&gt; output&quot;.</span>
        <span class="n">Dz</span> <span class="o">=</span> <span class="n">Htilde</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Add self.regularization component to gradient.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">Dz</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">nh</span>

        <span class="n">Htilde</span> <span class="o">=</span> <span class="n">Htilde</span> <span class="o">*</span> <span class="n">H</span>

        <span class="c1"># Gradient wrt bias terms in &quot;input =&gt; hidden&quot;.</span>
        <span class="n">Db</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">*</span> <span class="p">(</span><span class="n">Dc</span> <span class="o">-</span> <span class="n">Htilde</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

        <span class="n">xtilde</span> <span class="o">=</span> <span class="p">(</span><span class="n">diff</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">Z</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">Htilde</span> <span class="o">=</span> <span class="n">Htilde</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">/</span> <span class="n">nsamples</span>

        <span class="c1"># Gradient wrt linear coefs in &quot;input =&gt; hidden&quot;.</span>
        <span class="n">DW</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">xtilde</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Z</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">Htilde</span><span class="p">)</span>

        <span class="c1"># Add self.regularization component to gradient.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">DW</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">nh</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">ni</span>

        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="p">[</span><span class="n">DW</span><span class="p">,</span> <span class="n">Db</span><span class="p">,</span> <span class="n">Dz</span><span class="p">,</span> <span class="n">Dc</span><span class="p">]</span></div>

<div class="viewcode-block" id="neural_network.gradient_descent"><a class="viewcode-back" href="../../../slls/api/neural_network/neural_network.html#stat_learn_level_set.neural_network.neural_network.neural_network.gradient_descent">[docs]</a>    <span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Xtr</span><span class="p">,</span> <span class="n">ytr</span><span class="p">,</span> <span class="n">Xva</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">yva</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                         <span class="n">iters</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">update_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">ret_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Run `iters` gradient descent steps.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        Xtr: ndarray, shape=(nsamples, ninput)</span>
<span class="sd">            The training inputs -- examples by row.</span>

<span class="sd">        ytr: ndarray, shape=(nsamples,)</span>
<span class="sd">            The training outputs.</span>

<span class="sd">        Xva,yva: ndarrays, defaults=None</span>
<span class="sd">            These are the validation dataset analogues to `Xtr` and `ytr`.</span>
<span class="sd">            If given, then the MSE over this validation data is recorded.</span>

<span class="sd">        step: float </span>
<span class="sd">            Step size for gradient descent steps.</span>

<span class="sd">        iters: int, default=10</span>
<span class="sd">            Number of gradient descent steps to run.</span>

<span class="sd">        ret_last: bool, default=False</span>
<span class="sd">            If True, only the loss on the last iteration is returned.</span>

<span class="sd">        update_stats: bool, default=True</span>
<span class="sd">            If True, the training data `Xtr` are used to update the</span>
<span class="sd">            running empirical mean and standard deviation for automatic</span>
<span class="sd">            variable normalization.</span>

<span class="sd">        verbose: bool, default=True</span>
<span class="sd">            Print the loss(es) for each iteration.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        loss_tr[, loss_va]: ndarray[, ndarray], shapes=(iters+1,)</span>
<span class="sd">            The MSE over the training (and validation) sets. Note that</span>
<span class="sd">            these arrays are length `iters`+1 since the loss prior to the </span>
<span class="sd">            first gradient descent step is recorded as the first entry.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">update_stats</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">Xtr</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="n">q</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">iters</span><span class="p">))</span>
            <span class="n">pstr</span> <span class="o">=</span> <span class="s2">&quot;GD, ITER: </span><span class="si">%%</span><span class="s2">0</span><span class="si">%d</span><span class="s2">d, LOSSTR: </span><span class="si">%%</span><span class="s2">.5f&quot;</span> <span class="o">%</span> <span class="n">q</span>
            <span class="k">if</span> <span class="n">Xva</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">pstr</span> <span class="o">+=</span> <span class="s2">&quot;, LOSSVA: </span><span class="si">%.5f</span><span class="s2">&quot;</span>

        <span class="n">loss_tr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">iters</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">Xva</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss_va</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">iters</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">init_params</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
                       <span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="o">.</span><span class="n">copy</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iters</span><span class="p">):</span>
            <span class="n">loss_tr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">G</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lossgrad</span><span class="p">(</span><span class="n">Xtr</span><span class="p">,</span> <span class="n">ytr</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">Xva</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">loss_va</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">Xva</span><span class="p">,</span> <span class="n">yva</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="n">Xva</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="n">pstr</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">loss_tr</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
            <span class="k">elif</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="n">Xva</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="n">pstr</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">loss_tr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">loss_va</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">-=</span> <span class="n">step</span><span class="o">*</span><span class="n">G</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">-=</span> <span class="n">step</span><span class="o">*</span><span class="n">G</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">-=</span> <span class="n">step</span><span class="o">*</span><span class="n">G</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span> <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">-=</span> <span class="n">step</span><span class="o">*</span><span class="n">G</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

            <span class="c1"># If a bad loss value is encountered, reduce the step size.</span>
            <span class="c1"># hard-coded reduction of 0.9 and max tries of 100</span>
            <span class="n">bad_loss</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">for</span> <span class="n">loss_test_iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
                <span class="c1"># We need to check the loss after parameter update</span>
                <span class="c1"># to see if the step size caused a &#39;blow up&#39;.</span>
                <span class="n">ltr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">Xtr</span><span class="p">,</span> <span class="n">ytr</span><span class="p">)</span>
                <span class="n">lva</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">Xva</span><span class="p">,</span> <span class="n">yva</span><span class="p">)</span>

                <span class="c1"># Check if nan or if loss rose &gt;= relative factor of 100%.</span>
                <span class="n">bad_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">ltr</span><span class="p">)</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">lva</span><span class="p">)</span> <span class="ow">or</span> \
                           <span class="p">(</span><span class="n">ltr</span><span class="o">-</span><span class="n">loss_tr</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">/</span><span class="n">loss_tr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">2.0</span>

                <span class="k">if</span> <span class="n">bad_loss</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">loss_test_iter</span> <span class="o">==</span> <span class="mi">99</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="n">logger</span><span class="o">.</span><span class="n">warn</span><span class="p">((</span><span class="s2">&quot;Couldn&#39;t make a good descent step, &quot;</span>
                                         <span class="s2">&quot; returning initial parameters.&quot;</span><span class="p">))</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">*</span><span class="n">init_params</span><span class="p">)</span>

                        <span class="k">if</span> <span class="n">Xva</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="k">if</span> <span class="n">ret_last</span><span class="p">:</span>
                                <span class="k">return</span> <span class="n">loss_tr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="k">return</span> <span class="n">loss_tr</span><span class="p">[:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">if</span> <span class="n">ret_last</span><span class="p">:</span>
                                <span class="k">return</span> <span class="n">loss_tr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">loss_va</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="k">return</span> <span class="n">loss_tr</span><span class="p">[:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">loss_va</span><span class="p">[:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

                    <span class="c1"># Correct the parameters back.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">+=</span> <span class="n">step</span><span class="o">*</span><span class="n">G</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">+=</span> <span class="n">step</span><span class="o">*</span><span class="n">G</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">+=</span> <span class="n">step</span><span class="o">*</span><span class="n">G</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span> <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">+=</span> <span class="n">step</span><span class="o">*</span><span class="n">G</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

                    <span class="c1"># Reduce the step by factor of 0.9. </span>
                    <span class="n">step</span> <span class="o">*=</span> <span class="mf">0.9</span>

                    <span class="k">if</span> <span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">warn</span><span class="p">((</span><span class="s2">&quot;Bad loss encountered, reducing step &quot;</span>
                                     <span class="s2">&quot;size to </span><span class="si">%.7f</span><span class="s2">.&quot;</span><span class="o">%</span><span class="n">step</span><span class="p">))</span>

                    <span class="c1"># Perform grad desc update with smaller step.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">-=</span> <span class="n">step</span><span class="o">*</span><span class="n">G</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">-=</span> <span class="n">step</span><span class="o">*</span><span class="n">G</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">-=</span> <span class="n">step</span><span class="o">*</span><span class="n">G</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span> <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">-=</span> <span class="n">step</span><span class="o">*</span><span class="n">G</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># The loss was good, so we break the inner for loop</span>
                    <span class="c1"># to the gradient descent step (`i`) for loop.</span>
                    <span class="k">break</span>


        <span class="n">loss_tr</span><span class="p">[</span><span class="n">iters</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">Xtr</span><span class="p">,</span> <span class="n">ytr</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">Xva</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss_va</span><span class="p">[</span><span class="n">iters</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">Xva</span><span class="p">,</span> <span class="n">yva</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="n">Xva</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">pstr</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">loss_tr</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
        <span class="k">elif</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="n">Xva</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">pstr</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">loss_tr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">loss_va</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

        <span class="k">if</span> <span class="n">Xva</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">ret_last</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">loss_tr</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">loss_tr</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">ret_last</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">loss_tr</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">loss_va</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">loss_tr</span><span class="p">,</span> <span class="n">loss_va</span></div></div>

<div class="viewcode-block" id="stop_early"><a class="viewcode-back" href="../../../slls/api/neural_network/neural_network.html#stat_learn_level_set.neural_network.neural_network.stop_early">[docs]</a><span class="k">def</span> <span class="nf">stop_early</span><span class="p">(</span><span class="n">loss_hist</span><span class="p">,</span> <span class="n">hist_len</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dec</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns True when the linear trend over the `hist_len` most recent</span>
<span class="sd">    components of `loss_hist` is greater (or lesser if dec=False) than `eps`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">hist_len</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">hist_len</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
        
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss_hist</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">hist_len</span><span class="p">:</span>
        <span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss_hist</span><span class="p">[</span><span class="o">-</span><span class="n">hist_len</span><span class="p">:])</span>
        <span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="n">m</span><span class="p">),</span><span class="n">_</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">dec</span> <span class="ow">and</span> <span class="n">m</span> <span class="o">&gt;=</span> <span class="n">tol</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="n">dec</span> <span class="ow">and</span> <span class="n">m</span> <span class="o">&lt;=</span> <span class="n">tol</span><span class="p">):</span> 
            <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span></div>

<div class="viewcode-block" id="batch_queue"><a class="viewcode-back" href="../../../slls/api/neural_network/neural_network.html#stat_learn_level_set.neural_network.neural_network.batch_queue">[docs]</a><span class="k">class</span> <span class="nc">batch_queue</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given data, this implements a queue of batches.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bs</span> <span class="o">=</span> <span class="n">batch_size</span>

        <span class="c1"># x and y queues</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">xq</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">yq</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># only count the &quot;last&quot; element of the queue (the first element</span>
        <span class="c1"># of the list) if it is equal to the batch size.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">xq</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">xq</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">xq</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">bs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">0</span>

<div class="viewcode-block" id="batch_queue.update"><a class="viewcode-back" href="../../../slls/api/neural_network/neural_network.html#stat_learn_level_set.neural_network.neural_network.batch_queue.update">[docs]</a>    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Split x and y into batches and all the batches to the queue.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`x` and `y` must have same number of examples.&quot;</span><span class="p">)</span>

        <span class="c1"># If the last element of the Q is not full batch, then update it.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">xq</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">xq</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">bs</span><span class="p">:</span>
            <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">xq</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">xq</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">xq</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">bs</span><span class="o">-</span><span class="n">n</span><span class="p">]])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">yq</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">yq</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">bs</span><span class="o">-</span><span class="n">n</span><span class="p">]])</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bs</span><span class="o">-</span><span class="n">n</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
                <span class="k">return</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">bs</span><span class="o">-</span><span class="n">n</span><span class="p">:]</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">bs</span><span class="o">-</span><span class="n">n</span><span class="p">:]</span>

        <span class="c1"># Now split x and y into batches.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">bs</span><span class="p">:</span> <span class="c1"># Easy case, x and y don&#39;t make a full batch.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">xq</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">yq</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">nb</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">bs</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb</span> <span class="o">+</span> <span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">bs</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">xq</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">bs</span> <span class="p">:</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">bs</span><span class="p">])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">yq</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">bs</span> <span class="p">:</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">bs</span><span class="p">])</span></div>


<div class="viewcode-block" id="batch_queue.next"><a class="viewcode-back" href="../../../slls/api/neural_network/neural_network.html#stat_learn_level_set.neural_network.neural_network.batch_queue.next">[docs]</a>    <span class="k">def</span> <span class="nf">next</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the next batch (x,y) from the queue.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">xq</span><span class="o">.</span><span class="n">pop</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">yq</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The queue has no batches to offer.&quot;</span><span class="p">)</span></div></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h3><a href="../../../index.html">Table Of Contents</a></h3>
<p class="caption"><span class="caption-text">LIDC</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../lidc/setup.html">Setup and Dependencies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../lidc/create-dataset.html">Making a dataset for segmentation</a></li>
</ul>
<p class="caption"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../slls/examples/2d-hamburger.html">Segment 2d images</a></li>
</ul>
<p class="caption"><span class="caption-text">Full documentation and specs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../slls/api/index.html">SLLS API</a></li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  <li><a href="../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2018, Matt Hancock.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.7.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
    </div>

    

    
  </body>
</html>